{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       species  class_encoded\n",
      "0       Adelie              0\n",
      "1       Adelie              0\n",
      "2       Adelie              0\n",
      "4       Adelie              0\n",
      "5       Adelie              0\n",
      "..         ...            ...\n",
      "215  Chinstrap              1\n",
      "216  Chinstrap              1\n",
      "217  Chinstrap              1\n",
      "218  Chinstrap              1\n",
      "219  Chinstrap              1\n",
      "\n",
      "[214 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Load the penguin's dataset using the following code.\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the penguins dataset\n",
    "df = sns.load_dataset(\"penguins\")\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Filter rows for 'Adelie' and 'Chinstrap' classes\n",
    "selected_classes = ['Adelie', 'Chinstrap']\n",
    "df_filtered = df[df['species'].isin(selected_classes)].copy()  # Make a copy to avoid the warning\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Encode the species column\n",
    "y_encoded = le.fit_transform(df_filtered['species'])\n",
    "\n",
    "df_filtered['class_encoded'] = y_encoded\n",
    "\n",
    "# Display the filtered and encoded DataFrame\n",
    "print(df_filtered[['species', 'class_encoded']])\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "\n",
    "y = df_filtered['class_encoded']  # Target variable\n",
    "X = df_filtered.drop(['species', 'island', 'sex','class_encoded'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) What is the purpose of \"y_encoded = le.fit_transform(df_filtered['species'])\" ?\n",
    "To encode the species column into numeric values. This is necessary because the LogisticRegression model can only work with numeric data. 0 for Adelie and 1 for Chinstrap\n",
    "\n",
    "2) What is the purpose of \"X = df.drop(['species', 'island', 'sex'], axis=1)\" ?\n",
    "To create a new DataFrame that only contains the features that we want to use for the model.\n",
    "\n",
    "3) Why we cannot use \"island\" and \"sex\" features?\n",
    "They are not relevant to the classification of penguins into Adelie and Chinstrap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(171, 4) (43, 4) (171,) (43,)\n"
     ]
    }
   ],
   "source": [
    "#Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape , X_test.shape , y_train.shape , y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5813953488372093\n",
      "[[ 2.76035852e-03 -8.24299288e-05  4.72001671e-04 -2.87065965e-04]] [-8.44846925e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\uzuma\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Train the logistic regression model. Here we are using saga solver to learn weights.\n",
    "logreg = LogisticRegression(solver='saga')\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "print(logreg.coef_, logreg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Why is accuracy low? why does the saga solver perform poorly?\n",
    "\n",
    "Since, in this case, we used 171 training datasets, the poor performance of the saga solver is attributed to the limited number of datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "[[ 1.59665154 -1.42501103 -0.15238046 -0.003951  ]] [-0.0755452]\n"
     ]
    }
   ],
   "source": [
    "#Change the solver to \"liblinear\"\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "logreg.fit(X_train, y_train)\n",
    "# Predict on the testing data\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "print(logreg.coef_, logreg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Why is accuracy now? why does the \"liblinear\" solver perform better than \"saga\" solver ? \n",
    "\n",
    " The performance of the \"saga\" solver can be affected by the size of the dataset. For small datasets, \"liblinear\" perform better due to its simplicity, while \"saga\" require more data to perform well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (saga): 0.9767441860465116\n",
      "Accuracy (liblinear): 0.9767441860465116\n"
     ]
    }
   ],
   "source": [
    "#Repeat the above tasks after feature normalization and observe the accuracy levels.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "logreg = LogisticRegression(solver='saga')\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred = logreg.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy (saga):\", accuracy)\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred = logreg.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy (liblinear):\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Now observe the accuracies for both  \"liblinear\" solver and \"saga\" solver. Why accuracy of the \"saga\" solver is increased?\n",
    "\n",
    "Logistic regression is sensitive to the scale of features. Standard scaling can help logistic regression models, including the \"saga\" solver, perform better by ensuring that features are on similar scales. This scaling can lead to more stable and faster convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Dream'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10472\\4212251893.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\uzuma\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1147\u001b[0m                 skip_parameter_validation=(\n\u001b[0;32m   1148\u001b[0m                     \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m                 )\n\u001b[0;32m   1150\u001b[0m             ):\n\u001b[1;32m-> 1151\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\uzuma\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1203\u001b[0m             \u001b[0m_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1204\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1205\u001b[0m             \u001b[0m_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1207\u001b[1;33m         X, y = self._validate_data(\n\u001b[0m\u001b[0;32m   1208\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1209\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1210\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\uzuma\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    617\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m\"estimator\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcheck_y_params\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m                     \u001b[0mcheck_y_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdefault_check_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 621\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    622\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ensure_2d\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\uzuma\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1143\u001b[0m         raise ValueError(\n\u001b[0;32m   1144\u001b[0m             \u001b[1;34mf\"{estimator_name} requires y to be passed, but the target y is None\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1145\u001b[0m         )\n\u001b[0;32m   1146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1147\u001b[1;33m     X = check_array(\n\u001b[0m\u001b[0;32m   1148\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\uzuma\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 919\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m    920\u001b[0m                     \u001b[1;34m\"Complex data not supported\\n{}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    921\u001b[0m                 ) from complex_warning\n\u001b[0;32m    922\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\uzuma\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\_array_api.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[1;31m# Use NumPy API to support order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[1;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;31m# container that is consistent with the input's namespace.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\uzuma\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2082\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2083\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2084\u001b[1;33m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2085\u001b[0m         if (\n\u001b[0;32m   2086\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2087\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Dream'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Load the penguins dataset\n",
    "df = sns.load_dataset(\"penguins\")\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Filter rows for 'Adelie' and 'Chinstrap' classes\n",
    "selected_classes = ['Adelie', 'Chinstrap']\n",
    "df_filtered = df[df['species'].isin(selected_classes)].copy()  # Make a copy to avoid the warning\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Encode the species column\n",
    "y_encoded = le.fit_transform(df_filtered['species'])\n",
    "df_filtered['class_encoded'] = y_encoded\n",
    "\n",
    "\n",
    "df_filtered.head()\n",
    "\n",
    "X = df_filtered.drop(['species', 'class_encoded'], axis=1)  # Choose features\n",
    "y = df_filtered['class_encoded']  # Target variable\n",
    "\n",
    "X.head()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "logreg = LogisticRegression(solver='saga')\n",
    "\n",
    "#logreg = LogisticRegression(max_iter=166, solver='newton-cg')\n",
    "# logreg = LogisticRegression(penalty='l2', C=1.0, solver='lbfgs', max_iter=100, multi_class='ovr', random_state=42)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "print(logreg.coef_, logreg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the problem? Why algorithm cannot perform classification?\n",
    "How to solve this issue?\n",
    "\n",
    "‘Dream’ cannot be converted to a float. For logistic regression, only numerical values can be applied. In this case, there are some strings, so to solve this problem, we need to convert those string features into numerical values accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  species  bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g  \\\n",
      "0  Adelie            39.1           18.7              181.0       3750.0   \n",
      "1  Adelie            39.5           17.4              186.0       3800.0   \n",
      "\n",
      "   class_encoded  island_Dream  island_Torgersen  sex_Male  \n",
      "0              0         False              True      True  \n",
      "1              0         False              True     False  \n",
      "\n",
      "   species  bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g  \\\n",
      "0   Adelie            39.1           18.7              181.0       3750.0   \n",
      "20  Adelie            37.8           18.3              174.0       3400.0   \n",
      "\n",
      "    class_encoded  island_Dream  island_Torgersen  sex_Male  \n",
      "0               0         False              True      True  \n",
      "20              0         False             False     False  \n",
      "\n",
      "   species  bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g  \\\n",
      "0   Adelie            39.1           18.7              181.0       3750.0   \n",
      "30  Adelie            39.5           16.7              178.0       3250.0   \n",
      "\n",
      "    class_encoded  island_Dream  island_Torgersen  sex_Male  \n",
      "0               0         False              True      True  \n",
      "30              0          True             False     False  \n",
      "(214, 7) (214,)\n",
      "Accuracy: 1.0\n",
      "[[ 3.634131    0.16302678  0.62619229  0.1021229   2.59923914 -0.87720504\n",
      "  -0.35904767]] [-5.99599773]\n"
     ]
    }
   ],
   "source": [
    "#Use the following code, to replace string features with numerical ones.\n",
    "df_filtered = pd.get_dummies(df_filtered, columns=['island', 'sex'], drop_first=True)\n",
    "df_filtered.head()\n",
    "\n",
    "#Use the following code to visualize the encoding\n",
    "\n",
    "samples = df_filtered.groupby('sex_Male').head(1)\n",
    "print(samples)\n",
    "print()\n",
    "samples = df_filtered.groupby('island_Torgersen').head(1)\n",
    "print(samples)\n",
    "print()\n",
    "samples = df_filtered.groupby('island_Dream').head(1)\n",
    "print(samples)\n",
    "\n",
    "\n",
    "#Use the following code to apply logistic regression\n",
    "X = df_filtered.drop(['species','class_encoded'], axis=1)\n",
    "\n",
    "y = df_filtered['class_encoded']  # Target variable\n",
    "print(X.shape, y.shape)\n",
    "X.head()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "scaler=MaxAbsScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "logreg = LogisticRegression(solver='saga',max_iter=150,)\n",
    "\n",
    "#logreg = LogisticRegression(max_iter=166, solver='newton-cg')\n",
    "# logreg = LogisticRegression(penalty='l2', C=1.0, solver='lbfgs', max_iter=100, multi_class='ovr', random_state=42)\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred = logreg.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "print(logreg.coef_, logreg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why we are using the \"MaxAbsScaler\" scaler rather than the \"StandardScaler\"?\n",
    "\n",
    "Data doesn't have a Gaussian distribution, \"MaxAbsScaler\" can preserve the original scale of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g  \\\n",
      "14             34.6           21.1              198.0       4400.0   \n",
      "203            51.4           19.0              201.0       3950.0   \n",
      "72             39.6           17.2              196.0       3550.0   \n",
      "197            50.8           18.5              201.0       4450.0   \n",
      "123            41.4           18.5              202.0       3875.0   \n",
      "117            37.3           20.5              199.0       3775.0   \n",
      "20             37.8           18.3              174.0       3400.0   \n",
      "92             34.0           17.1              185.0       3400.0   \n",
      "81             42.9           17.6              196.0       4700.0   \n",
      "150            36.0           17.1              187.0       3700.0   \n",
      "188            47.6           18.3              195.0       3850.0   \n",
      "176            46.7           17.9              195.0       3300.0   \n",
      "147            36.6           18.4              184.0       3475.0   \n",
      "79             42.1           19.1              195.0       4000.0   \n",
      "184            42.5           16.7              187.0       3350.0   \n",
      "173            48.5           17.5              191.0       3400.0   \n",
      "196            50.9           17.9              196.0       3675.0   \n",
      "23             38.2           18.1              185.0       3950.0   \n",
      "51             40.1           18.9              188.0       4300.0   \n",
      "190            46.9           16.6              192.0       2700.0   \n",
      "104            37.9           18.6              193.0       2925.0   \n",
      "215            55.8           19.8              207.0       4000.0   \n",
      "217            49.6           18.2              193.0       3775.0   \n",
      "201            49.8           17.3              198.0       3675.0   \n",
      "154            51.3           19.2              193.0       3650.0   \n",
      "110            38.1           16.5              198.0       3825.0   \n",
      "35             39.2           21.1              196.0       4150.0   \n",
      "30             39.5           16.7              178.0       3250.0   \n",
      "181            52.8           20.0              205.0       4550.0   \n",
      "21             37.7           18.7              180.0       3600.0   \n",
      "61             41.3           21.1              195.0       4400.0   \n",
      "144            37.3           16.8              192.0       3000.0   \n",
      "164            47.0           17.3              185.0       3700.0   \n",
      "99             43.2           18.5              192.0       4100.0   \n",
      "75             42.8           18.5              195.0       4250.0   \n",
      "209            49.3           19.9              203.0       4050.0   \n",
      "177            52.0           19.0              197.0       4150.0   \n",
      "158            46.1           18.2              178.0       3250.0   \n",
      "103            37.8           20.0              190.0       4250.0   \n",
      "90             35.7           18.0              202.0       3550.0   \n",
      "107            38.2           20.0              190.0       3900.0   \n",
      "206            42.5           17.3              187.0       3350.0   \n",
      "66             35.5           16.2              195.0       3350.0   \n",
      "\n",
      "     island_Dream  island_Torgersen  sex_Male  \n",
      "14          False              True      True  \n",
      "203          True             False      True  \n",
      "72          False              True     False  \n",
      "197          True             False      True  \n",
      "123         False              True      True  \n",
      "117         False              True      True  \n",
      "20          False             False     False  \n",
      "92           True             False     False  \n",
      "81          False              True      True  \n",
      "150          True             False     False  \n",
      "188          True             False     False  \n",
      "176          True             False     False  \n",
      "147          True             False     False  \n",
      "79          False              True      True  \n",
      "184          True             False     False  \n",
      "173          True             False      True  \n",
      "196          True             False     False  \n",
      "23          False             False      True  \n",
      "51          False             False      True  \n",
      "190          True             False     False  \n",
      "104         False             False     False  \n",
      "215          True             False      True  \n",
      "217          True             False      True  \n",
      "201          True             False     False  \n",
      "154          True             False      True  \n",
      "110         False             False     False  \n",
      "35           True             False      True  \n",
      "30           True             False     False  \n",
      "181          True             False      True  \n",
      "21          False             False      True  \n",
      "61          False             False      True  \n",
      "144          True             False     False  \n",
      "164          True             False     False  \n",
      "99           True             False      True  \n",
      "75          False              True      True  \n",
      "209          True             False      True  \n",
      "177          True             False      True  \n",
      "158          True             False     False  \n",
      "103         False             False      True  \n",
      "90           True             False     False  \n",
      "107         False             False      True  \n",
      "206          True             False     False  \n",
      "66          False             False     False  \n",
      "[[0.59655172 0.98139535 0.93396226 0.91666667 0.         1.\n",
      "  1.        ]\n",
      " [0.8862069  0.88372093 0.94811321 0.82291667 1.         0.\n",
      "  1.        ]\n",
      " [0.68275862 0.8        0.9245283  0.73958333 0.         1.\n",
      "  0.        ]\n",
      " [0.87586207 0.86046512 0.94811321 0.92708333 1.         0.\n",
      "  1.        ]\n",
      " [0.7137931  0.86046512 0.95283019 0.80729167 0.         1.\n",
      "  1.        ]\n",
      " [0.64310345 0.95348837 0.93867925 0.78645833 0.         1.\n",
      "  1.        ]\n",
      " [0.65172414 0.85116279 0.82075472 0.70833333 0.         0.\n",
      "  0.        ]\n",
      " [0.5862069  0.79534884 0.87264151 0.70833333 1.         0.\n",
      "  0.        ]\n",
      " [0.73965517 0.81860465 0.9245283  0.97916667 0.         1.\n",
      "  1.        ]\n",
      " [0.62068966 0.79534884 0.88207547 0.77083333 1.         0.\n",
      "  0.        ]\n",
      " [0.82068966 0.85116279 0.91981132 0.80208333 1.         0.\n",
      "  0.        ]\n",
      " [0.80517241 0.83255814 0.91981132 0.6875     1.         0.\n",
      "  0.        ]\n",
      " [0.63103448 0.85581395 0.86792453 0.72395833 1.         0.\n",
      "  0.        ]\n",
      " [0.72586207 0.88837209 0.91981132 0.83333333 0.         1.\n",
      "  1.        ]\n",
      " [0.73275862 0.77674419 0.88207547 0.69791667 1.         0.\n",
      "  0.        ]\n",
      " [0.8362069  0.81395349 0.9009434  0.70833333 1.         0.\n",
      "  1.        ]\n",
      " [0.87758621 0.83255814 0.9245283  0.765625   1.         0.\n",
      "  0.        ]\n",
      " [0.65862069 0.84186047 0.87264151 0.82291667 0.         0.\n",
      "  1.        ]\n",
      " [0.69137931 0.87906977 0.88679245 0.89583333 0.         0.\n",
      "  1.        ]\n",
      " [0.80862069 0.77209302 0.90566038 0.5625     1.         0.\n",
      "  0.        ]\n",
      " [0.65344828 0.86511628 0.91037736 0.609375   0.         0.\n",
      "  0.        ]\n",
      " [0.96206897 0.92093023 0.97641509 0.83333333 1.         0.\n",
      "  1.        ]\n",
      " [0.85517241 0.84651163 0.91037736 0.78645833 1.         0.\n",
      "  1.        ]\n",
      " [0.85862069 0.80465116 0.93396226 0.765625   1.         0.\n",
      "  0.        ]\n",
      " [0.88448276 0.89302326 0.91037736 0.76041667 1.         0.\n",
      "  1.        ]\n",
      " [0.65689655 0.76744186 0.93396226 0.796875   0.         0.\n",
      "  0.        ]\n",
      " [0.67586207 0.98139535 0.9245283  0.86458333 1.         0.\n",
      "  1.        ]\n",
      " [0.68103448 0.77674419 0.83962264 0.67708333 1.         0.\n",
      "  0.        ]\n",
      " [0.91034483 0.93023256 0.96698113 0.94791667 1.         0.\n",
      "  1.        ]\n",
      " [0.65       0.86976744 0.8490566  0.75       0.         0.\n",
      "  1.        ]\n",
      " [0.71206897 0.98139535 0.91981132 0.91666667 0.         0.\n",
      "  1.        ]\n",
      " [0.64310345 0.78139535 0.90566038 0.625      1.         0.\n",
      "  0.        ]\n",
      " [0.81034483 0.80465116 0.87264151 0.77083333 1.         0.\n",
      "  0.        ]\n",
      " [0.74482759 0.86046512 0.90566038 0.85416667 1.         0.\n",
      "  1.        ]\n",
      " [0.73793103 0.86046512 0.91981132 0.88541667 0.         1.\n",
      "  1.        ]\n",
      " [0.85       0.9255814  0.95754717 0.84375    1.         0.\n",
      "  1.        ]\n",
      " [0.89655172 0.88372093 0.92924528 0.86458333 1.         0.\n",
      "  1.        ]\n",
      " [0.79482759 0.84651163 0.83962264 0.67708333 1.         0.\n",
      "  0.        ]\n",
      " [0.65172414 0.93023256 0.89622642 0.88541667 0.         0.\n",
      "  1.        ]\n",
      " [0.61551724 0.8372093  0.95283019 0.73958333 1.         0.\n",
      "  0.        ]\n",
      " [0.65862069 0.93023256 0.89622642 0.8125     0.         0.\n",
      "  1.        ]\n",
      " [0.73275862 0.80465116 0.88207547 0.69791667 1.         0.\n",
      "  0.        ]\n",
      " [0.61206897 0.75348837 0.91981132 0.69791667 0.         0.\n",
      "  0.        ]]\n",
      "[[-1.34082659  2.35505035  0.88068888  1.62029553 -1.14490646  1.80969611\n",
      "   1.01770049]\n",
      " [ 1.80078227  0.5480021   1.30057122  0.57562238  0.8734338  -0.55257896\n",
      "   1.01770049]\n",
      " [-0.40582395 -1.0008964   0.60076732 -0.35297599 -1.14490646  1.80969611\n",
      "  -0.98260737]\n",
      " [ 1.68858195  0.11775252  1.30057122  1.73637033  0.8734338  -0.55257896\n",
      "   1.01770049]\n",
      " [-0.069223    0.11775252  1.440532    0.40151018 -1.14490646  1.80969611\n",
      "   1.01770049]\n",
      " [-0.83592516  1.83875085  1.02064966  0.16936059 -1.14490646  1.80969611\n",
      "   1.01770049]\n",
      " [-0.7424249  -0.05434732 -2.47836983 -0.70120037 -1.14490646 -0.55257896\n",
      "  -0.98260737]\n",
      " [-1.4530269  -1.08694632 -0.93880125 -0.70120037  0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [ 0.21127779 -0.65669673  0.60076732  2.3167443  -1.14490646  1.80969611\n",
      "   1.01770049]\n",
      " [-1.07902585 -1.08694632 -0.65887969 -0.0047516   0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [ 1.09018027 -0.05434732  0.46080654  0.34347279  0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [ 0.92187979 -0.39854698  0.46080654 -0.93334996  0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [-0.96682553  0.0317026  -1.07876203 -0.52708818  0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [ 0.06167737  0.63405202  0.46080654  0.69169717 -1.14490646  1.80969611\n",
      "   1.01770049]\n",
      " [ 0.13647758 -1.43114598 -0.65887969 -0.81727517  0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [ 1.25848074 -0.74274665 -0.09903657 -0.70120037  0.8734338  -0.55257896\n",
      "   1.01770049]\n",
      " [ 1.70728201 -0.39854698  0.60076732 -0.062789    0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [-0.66762469 -0.22644715 -0.93880125  0.57562238 -1.14490646 -0.55257896\n",
      "   1.01770049]\n",
      " [-0.31232369  0.46195218 -0.51891891  1.38814594 -1.14490646 -0.55257896\n",
      "   1.01770049]\n",
      " [ 0.9592799  -1.5171959   0.0409242  -2.3262475   0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [-0.72372485  0.20380243  0.18088498 -1.80391093 -1.14490646 -0.55257896\n",
      "  -0.98260737]\n",
      " [ 2.62358459  1.23640143  2.1403359   0.69169717  0.8734338  -0.55257896\n",
      "   1.01770049]\n",
      " [ 1.46418132 -0.14039723  0.18088498  0.16936059  0.8734338  -0.55257896\n",
      "   1.01770049]\n",
      " [ 1.50158143 -0.91484648  0.88068888 -0.062789    0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [ 1.78208222  0.72010193  0.18088498 -0.1208264   0.8734338  -0.55257896\n",
      "   1.01770049]\n",
      " [-0.68632474 -1.60324582  0.88068888  0.28543539 -1.14490646 -0.55257896\n",
      "  -0.98260737]\n",
      " [-0.48062416  2.35505035  0.60076732  1.03992156  0.8734338  -0.55257896\n",
      "   1.01770049]\n",
      " [-0.424524   -1.43114598 -1.91852671 -1.04942476  0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [ 2.06258301  1.40850127  1.86041434  1.96851992  0.8734338  -0.55257896\n",
      "   1.01770049]\n",
      " [-0.76112495  0.28985235 -1.63860515 -0.23690119 -1.14490646 -0.55257896\n",
      "   1.01770049]\n",
      " [-0.08792305  2.35505035  0.46080654  1.62029553 -1.14490646 -0.55257896\n",
      "   1.01770049]\n",
      " [-0.83592516 -1.34509607  0.0409242  -1.62979873  0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [ 0.97797995 -0.91484648 -0.93880125 -0.0047516   0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [ 0.26737795  0.11775252  0.0409242   0.92384676  0.8734338  -0.55257896\n",
      "   1.01770049]\n",
      " [ 0.19257774  0.11775252  0.46080654  1.27207115 -1.14490646  1.80969611\n",
      "   1.01770049]\n",
      " [ 1.40808116  1.32245135  1.58049278  0.80777197  0.8734338  -0.55257896\n",
      "   1.01770049]\n",
      " [ 1.91298259  0.5480021   0.7407281   1.03992156  0.8734338  -0.55257896\n",
      "   1.01770049]\n",
      " [ 0.80967948 -0.14039723 -1.91852671 -1.04942476  0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [-0.7424249   1.40850127 -0.23899735  1.27207115 -1.14490646 -0.55257896\n",
      "   1.01770049]\n",
      " [-1.13512601 -0.31249707  1.440532   -0.35297599  0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [-0.66762469  1.40850127 -0.23899735  0.45954758 -1.14490646 -0.55257896\n",
      "   1.01770049]\n",
      " [ 0.13647758 -0.91484648 -0.65887969 -0.81727517  0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [-1.17252611 -1.86139557  0.46080654 -0.81727517 -1.14490646 -0.55257896\n",
      "  -0.98260737]]\n"
     ]
    }
   ],
   "source": [
    "#Use the following code to visualize feature scaling before and after normalization.\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "scaler=MaxAbsScaler()\n",
    "print(X_test)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(X_test_scaled)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you observe in the values related to \"island_Dream\",    \"island_Torgersen\"  and   \"sex_Male\" features before and after scaling?\n",
    "\n",
    "Before scaling they have true or false values, after scaling they have numerical values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
